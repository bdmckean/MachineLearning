{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSCI 5622 Fall 2017 HW#3\n",
    "## Brian McKean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Back Propagation (35pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework you’ll implement a feed-forward neural network for classifying handwritten dig- its. Your tasks will be to implement back propagation to compute the parameter derivatives for SGD and also do L2 regularization for SGD. First, make sure your code works on a small dataset(tinyTOY.pkl.gz) before moving on to lower-resolution version of MNIST(tinyMNIST.pkl.gz)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Programming questions (20 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finish nn.py. --- see code\n",
    "1. Finish back prop function to compute the weights and biases.   \n",
    "2. Finish SGD train function to do L2 regularization.  \n",
    "3. Add code to test on the tinyMNIST dataset.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Analysis (15 points)\n",
    "#### 1\\. What is the structure of your neural network (for both tinyTOY and tinyMNIST dataset)? Show the dimensions of the input layer, hidden layer and output layer.\n",
    "\n",
    "tinyToy dataset\n",
    "\n",
    "- Input Layer 2\n",
    "- Hidden layer 30\n",
    "- Output Layer 2\n",
    "\n",
    "tinyMIST\n",
    "- Input Layer 196\n",
    "- Hidden layer 30\n",
    "- Output Layer 10\n",
    "\n",
    "\n",
    "\n",
    "#### 2\\. What the role of the size of the hidden layer on train and test accuracy (plot accuracy vs. size of hidden layer using tinyMNIST dataset)?  \n",
    "\n",
    "```\n",
    ".    \n",
    ".     \n",
    ".     \n",
    ".     \n",
    ".     \n",
    ".     \n",
    ".     \n",
    ".     \n",
    ".     \n",
    ".     \n",
    ".     \n",
    ".     \n",
    ".     \n",
    ".       \n",
    ".     \n",
    ".     \n",
    "```  \n",
    "\n",
    "\n",
    "The number of nodes in the hidden layer helps with devloping greater accuracy up to a point.\n",
    "In this data analyis the best accuracy is achieved with a 30 node hidden layer and higher number of nodes do not significantly increase the test accuracy.\n",
    "\n",
    "Incresing the number of nodes too far cause the test accruacy to vary higher and lower indicating overfitting\n",
    "\n",
    "The number of nodes helps to reach the best accuracy faster as shown below.\n",
    "\n",
    "  \n",
    "#### 3\\. How does the number of epochs affect train and test accuracy (plot accuracy vs. epochs using tinyMINST dataset)?\n",
    "\n",
    "```\n",
    ".    \n",
    ".     \n",
    ".     \n",
    ".     \n",
    ".     \n",
    ".     \n",
    ".     \n",
    ".     \n",
    ".     \n",
    ".     \n",
    ".     \n",
    ".     \n",
    ".     \n",
    ".       \n",
    ".     \n",
    ".    \n",
    "\n",
    "```\n",
    "\n",
    "The number of epochs help to increase test accuracy.  With more nodes the higher test accuracies are acheived in fewer epochs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2 Keras CNN (35pts)\n",
    " \n",
    "Here, you will use the Conv2D layer in Keras to build a Convolutional Neural Network for the MNIST dataset. The input dataset is the same as the MNIST dataset in HW1, so you need to reshape the vector of each image into matrix for the use of Conv2D. And you need to build your model using the layers provided by Keras and achieve an accuracy higher than 98.5%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Programming questions (20pts)\n",
    "Finish the CNN.py to build a CNN model, train and improve your model to achieve 98.5% accuracy on MNIST dataset. (Hint: use one hot encoding for label, input for the final Dense layer need to be flattened, try Dropout layer to improve your model and don’t give up).\n",
    "1. Reshape your MNIST data.\n",
    "2. Finish     init     function to construct your model. \n",
    "3. Finish train function and fit to your training data.\n",
    "\n",
    "see code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Analysis (15pts)\n",
    "#### 1. Point out at least three layer types you used in your model. Explain what are they used for. \n",
    "\n",
    "```\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv2d_1 (Conv2D)            (None, 24, 24, 32)        832       \n",
    "_________________________________________________________________\n",
    "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 32)        0         \n",
    "_________________________________________________________________\n",
    "dropout_1 (Dropout)          (None, 12, 12, 32)        0         \n",
    "_________________________________________________________________\n",
    "flatten_1 (Flatten)          (None, 4608)              0         \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 1000)              4609000   \n",
    "_________________________________________________________________\n",
    "dropout_2 (Dropout)          (None, 1000)              0         \n",
    "_________________________________________________________________\n",
    "dense_2 (Dense)              (None, 10)                10010     \n",
    " \n",
    " =================================================================\n",
    "Total params: 4,619,842\n",
    "Trainable params: 4,619,842\n",
    "Non-trainable params: 0\n",
    "```\n",
    "\n",
    "- Convolution Layer -- slides filter over the 5x5 kernals with a strid of 1\n",
    "- Max_Pooling Layer -- make a single sample out if each 2x2 result\n",
    "- Dropout -- causes the previous results to be randomly dropped in each pass allowing more nodes to have influence in the result\n",
    "- Flatten layer -- takes the result down to a 1D array\n",
    "- Dense -- layer of nodes that have full connections\n",
    "- Final Dense Layer -- perepares data in form desired for outuput with 10 classifiers\n",
    "\n",
    "\n",
    "#### 2\\. How did you improve your model for higher accuracy?\n",
    "\n",
    "    - I added dropout to decrease the chances of overfitting.\n",
    "    - Then I increase epochs from 50 to 100 with a single 32 node convolution layer and got the desired result.\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    ".    \n",
    ".     \n",
    ".     \n",
    ".     \n",
    ".     \n",
    ".     \n",
    ".     \n",
    ".     \n",
    ".     \n",
    ".     \n",
    ".     \n",
    ".     \n",
    ".     \n",
    ".       \n",
    ".     \n",
    ".     \n",
    "```  \n",
    "\n",
    "    - I also tried a second convolutional layer of 64 nodes and was able to get to the desired result in 50 epochs.  Se results below\n",
    "\n",
    "________________________________________________________________\n",
    "\n",
    "```\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "    =================================================================\n",
    "conv2d_1 (Conv2D)            (None, 24, 24, 32)        832       \n",
    "_________________________________________________________________\n",
    "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 32)        0         \n",
    "_________________________________________________________________\n",
    "dropout_1 (Dropout)          (None, 12, 12, 32)        0         \n",
    "_________________________________________________________________\n",
    "conv2d_2 (Conv2D)            (None, 8, 8, 64)          51264     \n",
    "_________________________________________________________________\n",
    "max_pooling2d_2 (MaxPooling2 (None, 4, 4, 64)          0         \n",
    "_________________________________________________________________\n",
    "dropout_2 (Dropout)          (None, 4, 4, 64)          0         \n",
    "_________________________________________________________________\n",
    "flatten_1 (Flatten)          (None, 1024)              0         \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 1000)              1025000   \n",
    "_________________________________________________________________\n",
    "dropout_3 (Dropout)          (None, 1000)              0         \n",
    "_________________________________________________________________\n",
    "dense_2 (Dense)              (None, 10)                10010       \n",
    " \n",
    " \n",
    "  =================================================================\n",
    "\n",
    "\n",
    "Total params: 1,087,106  \n",
    "Trainable params: 1,087,106  \n",
    "Non-trainable params: 0  \n",
    "\n",
    "```\n",
    ".    \n",
    ".     \n",
    ".     \n",
    ".     \n",
    ".     \n",
    ".     \n",
    ".     \n",
    ".     \n",
    ".     \n",
    ".     \n",
    ".     \n",
    ".     \n",
    ".     \n",
    ".       \n",
    ".     \n",
    ".     \n",
    "```  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### 3\\. Try different activation functions and batch sizes. Show the corresponding accuracy.\n",
    "\n",
    "```\n",
    ".    \n",
    ".     \n",
    ".     \n",
    ".     \n",
    ".     \n",
    ".     \n",
    ".     \n",
    ".     \n",
    ".     \n",
    ".     \n",
    ".     \n",
    ".  \n",
    ".       \n",
    ".     \n",
    ".     \n",
    ".     \n",
    ".     \n",
    ".     \n",
    ".   \n",
    ".     \n",
    ".       \n",
    ".     \n",
    ".     \n",
    "```  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Keras RNN (30pts)\n",
    "Here you will use Keras to build a RNN model for sentiment analysis. You should use word embeddings and LSTM to finish LSTM.py. You will test your model on the IMDB dataset. And you are expected to achieve an accuracy higher than 90%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Programming questions (15pts)\n",
    "Finish the LSTM.py to build an RNN model. Use word embeddings as the first layer and use LSTM for sequential prediction.\n",
    "1. Preprocess data for LSTM (require data of the same length). \n",
    "2. Finish     init     function to construct your model.\n",
    "3. Finish train function and fit to your training data.\n",
    "\n",
    "Here is one f my better results\n",
    "```\n",
    "Using TensorFlow backend.\n",
    "dict_size=10000, example_length=512, embedding_length=128,  batch_size=4, epochs=15\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #\n",
    "=================================================================\n",
    "embedding_1 (Embedding)      (None, 512, 128)          1280000\n",
    "_________________________________________________________________\n",
    "dropout_1 (Dropout)          (None, 512, 128)          0\n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 64)                49408\n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 1)                 65\n",
    "=================================================================\n",
    "\n",
    "25000/25000 [==============================] - 5350s - loss: 0.4669 - acc: 0.7792 - val_loss: 0.3705 - val_acc: 0.8202\n",
    "Epoch 2/15\n",
    "25000/25000 [==============================] - 5300s - loss: 0.2429 - acc: 0.9028 - val_loss: 0.2671 - val_acc: 0.8930\n",
    "Epoch 3/15\n",
    "25000/25000 [==============================] - 5296s - loss: 0.1613 - acc: 0.9393 - val_loss: 0.2935 - val_acc: 0.8885\n",
    "Epoch 4/15\n",
    "25000/25000 [==============================] - 5309s - loss: 0.1064 - acc: 0.9622 - val_loss: 0.3304 - val_acc: 0.8858\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Analysis (15pts)\n",
    "#### 1\\. What is the purpose of the embedding layer? (Hint: think about the input and the output). \n",
    "\n",
    "The embedding layer build word vectors based on the data that you have. The input is a numercial representation of the word and the output vectors of the relationships between words\n",
    "\n",
    "#### 2\\. What is the effect of the hidden dimension size in LSTM?\n",
    "\n",
    "More nodes take longer to train and the accuracy rises over more epochs before platueauing.\n",
    "More nodes generally take longer to train and test.\n",
    "\n",
    "\n",
    "\n",
    "Here a variety of runs I did to check out how changing parameters affects the test accuracy.\n",
    "\n",
    "\n",
    "The highest test accuracy I achievd was with a small er number of nodes 64\n",
    "\n",
    "```\n",
    "\n",
    "Columns\n",
    "----\n",
    "run \n",
    "  dict_size\t\n",
    "        example_len\t\n",
    "            batch_size\t\n",
    "                embedding_len\t\n",
    "                    lstm_units\t\n",
    "                        Best Accuracy\t\n",
    "                            Best accuracy in this epoch (of 3 epochs)\n",
    "                            \n",
    "Changing example length        \n",
    "0\t5000\t128\t32\t64\t128\t0.8624\t2\n",
    "1\t5000\t256\t32\t64\t128\t0.8676\t2\n",
    "2\t5000\t512\t32\t64\t128\t0.8664\t2\n",
    "3\t5000\t768\t32\t64\t128\t0.8671\t3\n",
    "4\t5000\t1024\t32\t64\t128\t0.8682\t2\n",
    "Changing dict size\n",
    "5\t1000\t512\t32\t64\t128\t0.8490\t3\n",
    "6\t2500\t512\t32\t64\t128\t0.8622\t2\n",
    "7\t5000\t512\t32\t64\t128\t0.8707\t3\n",
    "8\t7500\t512\t32\t64\t128\t0.8613\t3\n",
    "9\t10000\t512\t32\t64\t128\t0.8519\t3\n",
    "Changing embedding length\n",
    "10\t5000\t512\t32\t16\t128\t0.8571\t3\n",
    "11\t5000\t512\t32\t32\t128\t0.8608\t3\n",
    "12\t5000\t512\t32\t64\t128\t0.8659\t3\n",
    "13\t5000\t512\t32\t128\t128\t0.8715\t3\n",
    "14\t5000\t512\t32\t256\t128\t0.8744\t2\n",
    "Changing batch size\n",
    "15\t5000\t512\t16\t64\t128\t0.8535\t3\n",
    "16\t5000\t512\t32\t64\t128\t0.8525\t3\n",
    "17\t5000\t512\t64\t64\t128\t0.8532\t1\n",
    "18\t5000\t512\t128\t64\t128\t0.8712\t2\n",
    "19\t5000\t512\t256\t64\t128\t0.8792\t2\n",
    "\n",
    "\n",
    "Changing LSTM units\n",
    "\n",
    "20\t5000\t512\t32\t64\t64\t0.8687\t2\n",
    "21\t5000\t512\t32\t64\t96\t0.8616\t3\n",
    "22\t5000\t512\t32\t64\t128\t0.8739\t2\n",
    "23\t5000\t512\t32\t64\t192\t0.8058\t1\n",
    "24\t5000\t512\t32\t64\t256\t0.8758\t3\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 3\\. Replace LSTM with GRU and compare their performance.\n",
    "```\n",
    "GRU performance was about the asme as LSTM with the same setup.\n",
    "The \n",
    "\n",
    "\n",
    "\n",
    "Here is an example\n",
    "dict_size=20000, example_length=512, embedding_length=128,  batch_size=32, epochs=15\n",
    "\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #\n",
    "=================================================================\n",
    "embedding_1 (Embedding)      (None, 512, 128)          2560000\n",
    "_________________________________________________________________\n",
    "dropout_1 (Dropout)          (None, 512, 128)          0\n",
    "_________________________________________________________________\n",
    "gru_1 (GRU)                  (None, 128)               98688\n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 1)                 129\n",
    "=================================================================\n",
    "\n",
    "25000/25000 [==============================] - 1311s - loss: 0.5039 - acc: 0.7541 - val_loss: 0.3929 - val_acc: 0.8264\n",
    "Epoch 2/15\n",
    "25000/25000 [==============================] - 1256s - loss: 0.3221 - acc: 0.8674 - val_loss: 0.3383 - val_acc: 0.8573\n",
    "Epoch 3/15\n",
    "25000/25000 [==============================] - 1459s - loss: 0.1999 - acc: 0.9250 - val_loss: 0.2929 - val_acc: 0.8843\n",
    "Epoch 4/15\n",
    "25000/25000 [==============================] - 1303s - loss: 0.1188 - acc: 0.9581 - val_loss: 0.3674 - val_acc: 0.8635\n",
    "Epoch 5/15\n",
    "25000/25000 [==============================] - 1376s - loss: 0.0713 - acc: 0.9763 - val_loss: 0.4488 - val_acc: 0.8546\n",
    "Epoch 6/15\n",
    "25000/25000 [==============================] - 1477s - loss: 0.0456 - acc: 0.9848 - val_loss: 0.4656 - val_acc: 0.8605\n",
    "Epoch 7/15\n",
    "25000/25000 [==============================] - 1281s - loss: 0.0335 - acc: 0.9884 - val_loss: 0.5412 - val_acc: 0.8617\n",
    "Epoch 8/15\n",
    "25000/25000 [==============================] - 1112s - loss: 0.0203 - acc: 0.9938 - val_loss: 0.6372 - val_acc: 0.8548\n",
    "Epoch 9/15\n",
    "25000/25000 [==============================] - 1105s - loss: 0.0192 - acc: 0.9938 - val_loss: 0.7120 - val_acc: 0.8537\n",
    "Epoch 10/15\n",
    "25000/25000 [==============================] - 1117s - loss: 0.0120 - acc: 0.9960 - val_loss: 0.6570 - val_acc: 0.8589\n",
    "Epoch 11/15\n",
    "25000/25000 [==============================] - 1092s - loss: 0.0127 - acc: 0.9957 - val_loss: 0.7253 - val_acc: 0.8550\n",
    "Epoch 12/15\n",
    "25000/25000 [==============================] - 1121s - loss: 0.0082 - acc: 0.9972 - val_loss: 0.7711 - val_acc: 0.8578\n",
    "Epoch 13/15\n",
    "25000/25000 [==============================] - 1091s - loss: 0.0079 - acc: 0.9970 - val_loss: 0.8059 - val_acc: 0.8573\n",
    "Epoch 14/15\n",
    "25000/25000 [==============================] - 1122s - loss: 0.0055 - acc: 0.9984 - val_loss: 0.8002 - val_acc: 0.8556\n",
    "Epoch 15/15\n",
    "25000/25000 [==============================] - 1179s - loss: 0.0058 - acc: 0.9982 - val_loss: 0.8742 - val_acc: 0.8565\n",
    "\n",
    "\n",
    "And another GRU example\n",
    "dict_size=20000, example_length=512, embedding_length=128,  batch_size=32, epochs=15\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #\n",
    "=================================================================\n",
    "embedding_1 (Embedding)      (None, 512, 128)          2560000\n",
    "_________________________________________________________________\n",
    "dropout_1 (Dropout)          (None, 512, 128)          0\n",
    "_________________________________________________________________\n",
    "gru_1 (GRU)                  (None, 128)               98688\n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 1)                 129\n",
    "=================================================================\n",
    "25000/25000 [==============================] - 1305s - loss: 0.5115 - acc: 0.7475 - val_loss: 0.3811 - val_acc: 0.8444\n",
    "Epoch 2/15\n",
    "25000/25000 [==============================] - 1227s - loss: 0.3575 - acc: 0.8498 - val_loss: 0.3368 - val_acc: 0.8539\n",
    "Epoch 3/15\n",
    "25000/25000 [==============================] - 1456s - loss: 0.2065 - acc: 0.9202 - val_loss: 0.2895 - val_acc: 0.8784\n",
    "Epoch 4/15\n",
    "25000/25000 [==============================] - 1316s - loss: 0.1224 - acc: 0.9551 - val_loss: 0.3285 - val_acc: 0.8743\n",
    "Epoch 5/15\n",
    "25000/25000 [==============================] - 1372s - loss: 0.0720 - acc: 0.9762 - val_loss: 0.4011 - val_acc: 0.8704\n",
    "Epoch 6/15\n",
    "25000/25000 [==============================] - 1472s - loss: 0.0430 - acc: 0.9853 - val_loss: 0.4521 - val_acc: 0.8607\n",
    "Epoch 7/15\n",
    "25000/25000 [==============================] - 1308s - loss: 0.0301 - acc: 0.9898 - val_loss: 0.5105 - val_acc: 0.8640\n",
    "Epoch 8/15\n",
    "25000/25000 [==============================] - 1114s - loss: 0.0180 - acc: 0.9946 - val_loss: 0.6100 - val_acc: 0.8609\n",
    "Epoch 9/15\n",
    "25000/25000 [==============================] - 1098s - loss: 0.0144 - acc: 0.9956 - val_loss: 0.6792 - val_acc: 0.8538\n",
    "Epoch 10/15\n",
    "25000/25000 [==============================] - 1116s - loss: 0.0123 - acc: 0.9959 - val_loss: 0.6622 - val_acc: 0.8522\n",
    "Epoch 11/15\n",
    "25000/25000 [==============================] - 1097s - loss: 0.0108 - acc: 0.9962 - val_loss: 0.7350 - val_acc: 0.8570\n",
    "Epoch 12/15\n",
    "25000/25000 [==============================] - 1119s - loss: 0.0116 - acc: 0.9959 - val_loss: 0.7120 - val_acc: 0.8472\n",
    "Epoch 13/15\n",
    "25000/25000 [==============================] - 1096s - loss: 0.0063 - acc: 0.9980 - val_loss: 0.7980 - val_acc: 0.8534\n",
    "Epoch 14/15\n",
    "25000/25000 [==============================] - 1119s - loss: 0.0054 - acc: 0.9981 - val_loss: 0.8704 - val_acc: 0.8454\n",
    "Epoch 15/15\n",
    "25000/25000 [==============================] - 1184s - loss: 0.0061 - acc: 0.9978 - val_loss: 0.9434 - val_acc: 0.8522\n",
    "25000/25000 [==============================] - 359s\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extra credits (5pts) Try to use pretrained word embeddings to initialize the embedding layer and see how that changes the performance.\n",
    "\n",
    "I did not do the extra credit. I spent all my time this week on the LSTM trying to get to 90%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:hwenv]",
   "language": "python",
   "name": "conda-env-hwenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
